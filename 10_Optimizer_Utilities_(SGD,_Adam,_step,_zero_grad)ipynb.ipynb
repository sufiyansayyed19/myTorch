{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5/69ySqC49ImysCyNWFSU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sufiyansayyed19/myTorch/blob/main/10_Optimizer_Utilities_(SGD%2C_Adam%2C_step%2C_zero_grad)ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Goal\n",
        "\n",
        "Provide a clear, example-driven reference for PyTorch optimizer utilities so parameter updates, learning rate usage, and optimizer mechanics are unambiguous.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Understanding tensors and parameters.\n",
        "Basic idea that optimizers update parameters using gradients.\n",
        "\n",
        "## After This Notebook You Can\n",
        "\n",
        "Choose and configure common optimizers.\n",
        "Explain optimizer.step() and optimizer.zero_grad().\n",
        "Pass parameters correctly to optimizers.\n",
        "Answer optimizer-related interview questions confidently.\n",
        "\n",
        "## Out of Scope\n",
        "\n",
        "Optimizer math derivations.\n",
        "Learning rate schedules.\n",
        "Advanced optimization strategies.\n",
        "\n",
        "---\n",
        "\n",
        "## METHODS COVERED (SUMMARY)\n",
        "\n",
        "Optimizers:\n",
        "\n",
        "* torch.optim.SGD\n",
        "* torch.optim.Adam\n",
        "\n",
        "Core calls:\n",
        "\n",
        "* optimizer.step\n",
        "* optimizer.zero_grad\n",
        "\n",
        "Configuration:\n",
        "\n",
        "* learning rate (lr)\n",
        "* momentum (SGD)\n",
        "* weight_decay\n",
        "\n",
        "---\n",
        "\n",
        "## torch.optim.SGD\n",
        "\n",
        "What it does:\n",
        "Updates parameters using stochastic gradient descent.\n",
        "\n",
        "When to use:\n",
        "Simple models, strong baselines, when you want explicit control.\n",
        "\n",
        "Minimal example:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = nn.Linear(1, 1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "```\n",
        "\n",
        "Important parameters:\n",
        "\n",
        "* lr: learning rate\n",
        "* momentum (optional)\n",
        "* weight_decay (optional)\n",
        "\n",
        "Common mistake:\n",
        "Forgetting to pass model.parameters().\n",
        "\n",
        "---\n",
        "\n",
        "## torch.optim.Adam\n",
        "\n",
        "What it does:\n",
        "Adaptive optimizer that adjusts learning rates per parameter.\n",
        "\n",
        "When to use:\n",
        "Most deep learning tasks as a strong default.\n",
        "\n",
        "Minimal example:\n",
        "\n",
        "```python\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "```\n",
        "\n",
        "Important parameters:\n",
        "\n",
        "* lr\n",
        "* betas\n",
        "* weight_decay\n",
        "\n",
        "Common mistake:\n",
        "Assuming Adam removes the need to tune learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "## optimizer.zero_grad\n",
        "\n",
        "What it does:\n",
        "Clears accumulated gradients.\n",
        "\n",
        "When to use:\n",
        "Before each backward pass.\n",
        "\n",
        "Minimal example:\n",
        "\n",
        "```python\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "Common mistake:\n",
        "Calling zero_grad() after backward().\n",
        "\n",
        "---\n",
        "\n",
        "## optimizer.step\n",
        "\n",
        "What it does:\n",
        "Updates parameters using current gradients.\n",
        "\n",
        "When to use:\n",
        "After backward() has computed gradients.\n",
        "\n",
        "Minimal example:\n",
        "\n",
        "```python\n",
        "optimizer.step()\n",
        "```\n",
        "\n",
        "Common mistake:\n",
        "Calling step() without computing gradients.\n",
        "\n",
        "---\n",
        "\n",
        "## Weight Decay (Conceptual)\n",
        "\n",
        "What it does:\n",
        "Applies L2 regularization during parameter updates.\n",
        "\n",
        "When to use:\n",
        "Preventing overfitting.\n",
        "\n",
        "Minimal example:\n",
        "\n",
        "```python\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "```\n",
        "\n",
        "Common mistake:\n",
        "Confusing weight_decay with dropout.\n",
        "\n",
        "---\n",
        "\n",
        "## HANDS-ON PRACTICE\n",
        "\n",
        "1. Create a simple nn.Linear model and attach an SGD optimizer.\n",
        "2. Switch to Adam and compare required learning rates.\n",
        "3. Simulate a training step: zero_grad → backward (mock) → step.\n",
        "4. Explain why gradients accumulate if zero_grad is skipped.\n",
        "\n",
        "---\n",
        "\n",
        "## METHODS RECAP (ONE PLACE)\n",
        "\n",
        "SGD, Adam, optimizer.step(), optimizer.zero_grad(), lr, momentum, weight_decay\n",
        "\n",
        "---\n",
        "\n",
        "## ONE-SENTENCE SUMMARY\n",
        "\n",
        "Optimizers update parameters using gradients, but only when called in the correct order.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1Y0ASf48ryUX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2f88c8f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Simple model for demonstration\n",
        "model = nn.Linear(10, 1)\n",
        "# Mock input and target\n",
        "input_data = torch.randn(1, 10)\n",
        "target = torch.randn(1, 1)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f298b2f",
        "outputId": "8f3dbc80-808d-4670-b0e2-3f410abd12c9"
      },
      "source": [
        "# torch.optim.SGD example with lr, momentum, and weight_decay\n",
        "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n",
        "print(\"SGD Optimizer initialized.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD Optimizer initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad1677dc",
        "outputId": "6fa23200-dac2-4156-d680-a892bccf3ecd"
      },
      "source": [
        "# torch.optim.Adam example with lr and weight_decay\n",
        "optimizer_adam = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "print(\"Adam Optimizer initialized.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam Optimizer initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ca96ad5",
        "outputId": "95703952-a362-47e3-ec01-3b31b36b36ee"
      },
      "source": [
        "# optimizer.zero_grad example\n",
        "# This clears existing gradients so they don't accumulate\n",
        "optimizer_adam.zero_grad()\n",
        "print(\"Gradients cleared.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "232e509f",
        "outputId": "550677cd-6254-454b-ec17-903d294178fe"
      },
      "source": [
        "# optimizer.step example\n",
        "# 1. Perform a forward pass\n",
        "output = model(input_data)\n",
        "loss = criterion(output, target)\n",
        "\n",
        "# 2. Backward pass to compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# 3. Update parameters using the gradients\n",
        "optimizer_adam.step()\n",
        "print(\"Model parameters updated.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters updated.\n"
          ]
        }
      ]
    }
  ]
}